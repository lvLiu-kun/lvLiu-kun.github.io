# 话费充值

老板之前是做实体行业的，小富一代，捣了一手资源，自己搞了话费充值系统，中间商赚差价，公司自己的系统。
开发人员：
推单系统 3 个后端 + 1 个前端；
配单系统 3 个后端 + 1 个前端。

Tps 和 Qps 是多少？稳定支持 Tps 200，Qps 1000，最高 Tps 600，峰值 Qps 1500；
订单量？一秒钟几百单，一天可能会有 10 W+，一个月最高 500 W 单。

账怎么对的？有个 B 端管理系统，负责对账，看每天跑了多少流水，多少单成了，多少单没成，赚了多少钱。但我们一般都是线下对，因为线上比较难搞，上游下游这样去对，有些账是对不明白的，比如上游推超时了，其实我们拿到单子了，结果我们走完流程了，去回调，上游那没那单子，因为上游推失败了，推给其他人了，就有这种莫名其妙的场景，说不清，所以一般这种异常单子都会记录下来，线下自己有这个运营人员拿着这个流水号去核对；以及上游单子给我们多少钱，我们又给下游多少钱，这钱怎么分，这些运营人员都会和老板提前商量好，配在自己的后台系统里，之后每个月底或月初去对一次账（打钱是向账户打）。

为什么离职？公司有加班文化，大家为了加班而加班，因此团队的工作效率比较低，我还是希望可以加入一个高效的团队，大家为了工作目标而努力；还有就是公司的条薪制度不合理，导致员工的工作热情低（努力或者不努力得到的报酬都有一样的）。

## 充值流程

c 端用户在平台商家充值话费，然后平台商家把单子推给我们话费充值系统，然后我们去运营商代理那拉取支付凭证，之后下游来调用我们的接口拉取支付凭证和单子信息进行支付。

## 职责梳理

我们就是一个中间商，只是捣了一手资源赚差价，公司的业务能力就是和运营代理商拉上关系，然后对接平台商家，拉单后推单给下游支付，支付成功后我们就进行回调平台商家，平台商家会发通知给用户，运营代理商也会发短信给用户。
注：我们只负责单据的流转状态，是不涉及钱的，有一个专门的线上对账系统去对账。

下游的业务能力就是拥有自己的优惠支付渠道。

平台商家的业务能力就是搞流量，拿用户的单子。

接入新的平台商家流程：我们后台系统开通一个账号，生成对应的 appId 和 appKey，上游通过这个生成验签（appId + appKey + md5 加密），然后通过验签调用我们接口进行推单。
接入新的下游支付渠道流程：和上面是一样的流程。

## 系统业务流程

上游给我们推单，推单系统落库完后推给配单系统，配单系统落库完之后去运营代理商拉取支付凭证，拿到之后直接给下游，下游是通过调用我们接口把数据拿过去，支付成功后回调配单系统，然后配单系统回调推单系统，再然后推单系统回调上游。

注：
推单系统和配单系统都有落库操作，防止 Redis 宕机造成数据丢失，每走一步，都更新一下内部的库表的状态，哪怕更新失败了，还可以重试。
支付凭证过期时间 3 分钟。

## 充值系统模块

### 推单系统

接上游的单子，然后推给我们自己的配单系统。

记账：记需要支付的金额和上游的费率（差价）。

### 配单系统

配单系统的数据来源不光是我们的推单，也有对接其他的系统。

而且支付凭证这件事情，我们要去对比下游谁更赚钱，因为它们的策略会动态的变化，差价一直在变（有些单子是我们自己拉取支付链接去支付的）；还要算下它们的成功率，因为单子的成功率不是很高，大概只有 30 %，我们也不清楚下游具体是怎么充值的，充值逻辑是什么样的，我们只是做其中的一环，我们看到的数据量就是 30 %，它们就是有支付手段，感觉像灰色产业，所以有时失败率会很高，这也是为什么我们会出现 oom 这种问题，所以我们有单独的策略系统，去配它的成功次数，它的一个利润差价，然后再去决定这个订单怎么处理。

记账：记录要分账的金额和下游的费率（差价）。

5:49

### 推送服务

专门和三方系统进行 http 交互的服务。

为什么要单独拆一个推送服务处来：
和下游只能通过调用接口的方式交互；
不把 Redis 暴露给下游；
分摊机器压力；
职责解耦。

### 后台服务

## 迭代过程

### 同步的 http 调用

缺点：
很慢，tps 达不到平台商家要求，上线要求达到 100 Tps，当时项目只能承受 10 Tps左右。
网络抖动容易导致请求阻塞，如果我们拉运营代理商的支付凭证快了，或者上下游拉我们的接口快了，再或者推单系统和配单系统有延迟了，直接一个网络抖动把整个项目系统拖垮了。

### 把推送服务单独拆出来

线程池异步调用三方接口。

### Okhttp 发送异步的 http 请求

### Redis 进行异步

用消息队列，当时我们用的是 Redis，因为当时项目到处都在用 Redis，用 Redis 搞这个也简单；如果用 Rocket MQ 就会增加维护成本和开发人员学习成本。

提示：消息队列的作用。

## 项目难点

### 数据库没有建立合适的索引

每个订单过来都有它自己的状态，如支付状态、回调状态等，都要加好索引。
比如推单系统一个单子进来了，有落库状态、推单状态；配单系统拉取支付凭证状态、推单状态、下游回调状态。

### OOM

一开始都是同步的 http 调用，调用三方接口没做任何限流、缓冲处理，直接同步调用，当并发量上来了，调用的第三方接口可能数据库或者机器的压力很大，处理速度变得很慢，还有如果请求的第三方接口链路过长，以及偶发的网络抖动，都会导致请求直接阻塞，把整个服务都压垮了，一单一单的跑，跑个一二十单就不行了；所以同步的 http 调用，请求容易阻塞，导致用户连接池越滚越大，最后把连接占满了，线程搞了几千个出来，就一两分钟的事，整个系统直接爆了，OOM了。

怎么排查出来的？
我们这个 OOM 比较特殊，服务一打挂，一看日志全是 ReadTimeOutException，线程 ID 都到几百去了，很明显调用超时了，所以可以直接定位造成 OOM 原因的代码块；
最后一条日志是 OutOfMemoryException。
注：生成或者分析 dump 文件参考 JVM 调优。

当时有瓶颈了，想着要么拆系统要么拆服务，要么搞异步要么搞批量进行处理，所以后面把推送服务单独拆出来了，用线程池异步调用三方接口，发现又有问题，当时配的线程池参数太大了（当时配置在 yaml 文件里，没人注意看），1000 个核心线程数，5000 个等待队列（跟无界队列没什么区别了），只要还出现上述什么问题，还是会导致请求直接阻塞，一直 new 线程，new 线程是有开销的，线程栈内存默认是 1 M，可能还没到 1000 就 oom 了。
注：资损几千块钱，一万块钱应该有，那时业务还不大，本身也是下游造成的问题。
解决方案：等待队列改成 500，请求超时时间 3 分钟改为 5 秒，改用 okhttp 发送异步 http 请求。

后面为了进一步提高并发量，我们想到了用消息队列进行异步，我们用的是 Redis，后面都是用 Redis 进行异步。

### 死锁

总结性发言：当时在优化接口，拆了线程池异步批量去跑，然后涉及到同一行不同状态字段的的修改，因为修改的顺序不是一致的，所以出现死锁了。

具体问题的业务流程：当推单系统把单子一路推往下游了，然后下游回调到配单系统，配单系统又回调到推单系统时，推单系统要做两件事：
批量更改配单系统回调状态；
通知上游成功后再批量（200 条？）更改订单状态。
之前这两件事是同步处理的，很慢，后面把这个两个件事改成异步处理，直接往线程池里提交两个异步任务。调用接口通知上游时也是通过 Redis + 推送服务实现的，上游成功之后，然后推送服务又把结果写到 Redis 里面，推单系统有一个定时任务异步线程去 Redis 里面拉取结果，批量的修改订单状态。因为推给上游的顺序和它处理的顺序可能不一致，这个时候批量更改配单系统回调状态和上游成功后再批量更改订单状态可能同时处理同一批数据，发生了死锁情况。
示例：
update xx set where id = a;
update xx set where id = b;
update xx set where id = c;
update xx set where id = c;
update xx set where id = b;
update xx set where id = a;
解决方案：
排序，按照订单号顺序执行，但是我们这里不采用这个方案，因为一张表有很多状态字段，每一个更新场景，都要排序，假如后面又新增了其他状态字段，或者又新增了其他系统，万一开发人员他不知道或者忘了排序，还是会有死锁问题，比较强依赖开发，把避免死锁的逻辑侵入到业务代码里面去了。
示例（注：Mysql 优化器会进行优化）：
update xx set where id in (a, b, c);
update xx set where id in (c, b, a);
热点数据的垂直分表，再单独拆一张表出来，从根源上解决了这种并发冲突了，扩展性比较强；并且可以提高并发效率，因为如果不拆表，即使用排序解决了死锁问题，但是需要等待行锁的释放（事务的结束）。
怎么发现的？查看日志，发现 DiedLockException，说明发生了死锁；然后去阿里云 MySQL 看，图形化界面会显示哪里发了死锁。

### Redis 使用 list 存储消息左进右出还是左进左出的问题

总结性发言：因为业务特殊性，不知道什么情况，下游动不动就不来拉了，不能正常被消费。

一开始是左进右出，后面是左进左出。
左进右出：当消息量大的时候，下游处理不过来，有瓶颈，假设 list 里面的数据都快要过期的时候，先处理前面先进来的，如果处理它一半的时候发现过期了，然后去处理后面的，有可能把后面后进来的数据拖过期了，如果后面后进来的能处理，但是因为前面先进来的占用了处理时间，所以不能得到处理（订单超时）。一般情况下下游和我们的能力还是匹配的，但就是防不住它抽风。
左进左出：为了能抢救几个抢救几个，我们先处理后面后进来的，假设它快要过期了，但是刚好能处理完它，那还能抢救下，如果它都不能处理完，那前面先进来的也不能处理完。
过期消费失败的消息有定时任务查单子，如果哪一步出现异常了，就重试。

延伸点：
如果你要去调第三方的系统，如何和下游协调这件事，要考虑哪些点？
问清楚 QPS、TPS 能提供多少，拒绝策略、降级熔断策略是什么，按它们规则去配好。
下游处理太慢导致消息积压，要做好应对策略，比如把数据放 Redis 、MySQL 都行。
如果是别人来调我们系统，我们要考虑哪些点？
问清楚上游 QPS、TPS 的需求是多少，我们要考虑能提供多少，什么界限是安全的，针对自己接口和服务消费的东西做好限流或者策略。
做好兜底，防御性编程，如：上游云服务的消息队列出问题了，本来说好上游发的是唯一的，结果发了一堆重复数据过来，我们就懵了，出了安全生产事故（最后把数据库里的脏数据删了只保留最原始一条）；所以作为下游，不管上游如何给你承诺，自己数据库的数据保障不能交给上游，即使上游做好唯一性校验了，我们数据库也必须做好幂等进行兜底，加上唯一索引。
1:07:29

### Redis set 结构

总结性发言：避免网络抖动触发推单系统的 http 重试，导致推了很多重复的单子。

背景：当时是因为业务的特殊性，项目还在初期，老板为了省钱，用了几个便宜的云服务，一个是阿里云，另一个是腾讯云，网络有时很不稳定，很容易失败重试，一直推重复的单子，就改用 set 了；任何时候都有可能重试的机率，只不过当时这里刚好问题比较严重；后面业务好起来了，买了好一点的云服务，就没事了。



### token 重复刷新问题

背景：通过 token 去调用第三方接口是要花钱的，token 每刷新一次 800 元（100？），30 分钟过期。

场景 1：并发比较高的时候，当 token 过期的一瞬间，如果同时进入 100 个请求，就同时去刷新了，本来只要刷新一次 token。

解决方案：reentrantlock + 自旋锁
一开始考虑的是加锁，虽然加锁能保证不会同时去刷新，但是它们会顺序去刷新；后面改成加锁后还要进行 double check；为了防止线程没抢到锁一直等待，我们还设置了一个 200 ms 的超时时间，以及为了防止先抢到锁的线程刷新失败后，后面线程加锁超时后直接就返回成功了，我们还加入了自旋机制。

整个流程是这样的：
当发现 token 过期（调用接口报错或者异常，即请求不通了）（first check），首先 tryLock 设置 200 ms超时时间（线程不能一直等着）（约是刷新 token 所需要的时间），然后进行第二次判定（借鉴 单例模式 的 double check 思想），如果 token 刷新时间(Redis 记录了token 及其刷新时间) < 当前线程记录的 token 过期时间则更新 token，否则直接返回成功；然后为了防止先抢到锁的线程刷新失败（接口调不通了），后面线程加锁超时后直接就返回成功了，所以为了提高成功率，自旋 3 次（while 循环），如果还超时，那就直接返回。

注：一开始是单体项目，用的是 reentrantlock；后面改成分布式，用的是 Redis 分布式锁。

代码示例：
if(token 是否过期) {
	while (自旋次数 < 3) {
		tryLock(200);
		if(刷新时间 < 当前线程记录的过期时间) {
			刷新
		} else {
			返回成功
		}
	}
}
场景 2：请求失败这个时间点和真正记录过期时间点不是原子的，极限情况下：
1:00 请求失败；
1:01 被刷新了；
1:02 记录过期时间。
加锁之后，1:02 发现失败，1:01 刚刷新完，还是不行（这段逻辑和后面的更新逻辑不是原子的）。
解决方案：请求之前记录一下时间。

### Redis 宕机或者系统重启了

因为我们做了这种异步批量操作，在效率和和保障中选择了效率，所以如果数据没了导致订单状态流转停滞了，我们后台还配了定时任务去兜底，扫描最近的单子看看是不是有状态长期都没有更新的数据，看是在哪一步把数据丢了，导致它没有继续往前走，这种单子通过补偿定时任务再给它继续走流程，

### 推单系统和配单系统的数据（最终）一致性

具体问题的业务流程：推单系统推单到 Redis 里，配单系统从 Redis 里面拉取单子。

此时：
推单系统可能调用 Redis 失败；
解决方案：本地 catch 异常，然后重试。
配单系统可能拉取 Redis 失败；
解决方案：重新拉取。
或者配单系统拉取 Redis 数据成功，但是消费失败。
解决方案：由于没有 ack 机制，拉取完 Redis，Redis 里面的数据就没了，所以作为下游，这时候一般来说不管怎么样，我们都不会做任何操作，先保证落库，再慢慢消费，消费具体逻辑可以异步（定时任务扫表）。
提示：参考 Rocket MQ 消息丢失导致的数据不一致性解决方案。

注：即使把这些问题考虑到位了，还是有可能出现极端情况，比如：推单系统就是一直重试失败，或者机器宕机了、重启了，再或者 Redis 崩掉了，持久化机制没有把数据持久化到磁盘；不管怎样防御极端情况，机器宕机一定会有数据丢失风险，造成不一致，各种各样的情况我们都要考虑，所以我们这就有一个兜底策略，就是定时任务查询重试，比如推单系统到推单系统正常 2 分钟应该走完流程，这时候配单系统应该给推单系统回调，开个定时任务 2 分 10 秒时统一查询那些没有收到配单系统回调的数据，只要没收到回调（有个字段用来比较时间），统一认为失败，然后另一个定时任务通过状态用 where 条件把没有成功的数据过滤出来，然后又重发 Redis。重发数据重复了怎么办？下游做好幂等就行了，作为下游，不管上游如何给你承诺，一定要做好幂等（不然就是你的生成事故）；重发一直失败怎么办？看公司具体业务，重试试几次，我们项目最简单三次，三次还是不行，报警，飞书+短信+人工业务（程序员半夜起来解决问题）。


### 数据归档

订单量？一秒钟几百单，一天可能会有 10 W+，一个月最高 500 W 单（10 月或者 11 月，有特殊节日，如国庆节）（正常单量 300 W 左右）。

由于每月都会产生百万条订单，数月积压后会变成千万级大表，导致查询效率极具下降，因此我们每月 3 号（按时间分）（避开月初的充值高峰期）都有一个定时任务进行数据归档，将上个月以前的数据迁移到新表中（表名：order_info + 年月日）（先将数据插入新表，然后删除旧表数据，插入删除逻辑放在同一个事务中，避免数据不一致），每 10 秒迁移 500 条，慢慢迁移，主表只保留两个月的数据供查询使用。

注：这种不适合 to-C 的业务，因为所有历史记录都是有业务意义的，就要分库分表了。

## Redis

### token（string）

### 支付凭证（list）

### 订单信息（set）

### 上下游利润分配比例

不用保持长期更新，不同钱的单子不同的利润分配比例，每个单一来一成功都要去记账，方便对账。

### 存储系统参数（hash）

### 中间商的信息

### 上下游信息

调用三方接口的秘钥

## 缓存刷新一致性

双写一致性：更新数据库和 Redis 数据，通过 Canal 保证；读写一致性：并发读然后再写的情况下，数据的一致性，主要是 double check。

场景：
多线程读取 MySQL 数据（读取的数据不一样）；
然后写入 Redis（线程写入顺序和读的顺序不一样）。

读缓存逻辑：
从 Redis 里面读，有则直接返回；没有则（first check）
加锁（读写不一致是因为读写不是原子性的）；
double check（只能有一个人去更新）；
然后从 MySQL 读；
然后写到 Redis。

## Rocket MQ

订单超时逻辑：如果下游支付渠道消费成功但是没有回调，只要没回终态，有个 Rocket MQ 30 分钟的延时消息，主动去查询一下下游支付渠道的订单状态进行回调上游。

## 三种异步异步方式

### 定时任务扫 MySQL

下游回调的数据存到 MySQL，然后用定时任务每 2 秒去扫 500 条（消费表）（最多 500 条，太多了定时任务会阻塞）。

注：下游支付渠道回调和配单系统回调时，我们都会记录记录表和消费表（往里面插一样的数据）。
用 MySQL 做异步为什么能快？就是因为我们拆了两张表，一张记录表，一张消费表，用消费表做异步，用 delete 消费表替换 update 记录表。如果只有一张记录表，每次回调都会插入，那这张表的数据会越来越多，变成大表，消费完之后 update 一下状态（小表 update 当然可以，百万级数据就很慢了），就会很慢，所以搞了两张表，记录表只负责插入和查询，消费表只负责插入和删除，消费完就 delete，所以能保证消费表永远是张小表，最多也只是 1000 以内的数据，在这个业务场景下用 delete 替换 update 性能就会很高，这也是为什么我们敢用 MySQL 做异步的原因。

为什么不用 Redis 做异步？落库之后涉及到对库状态更改。

### 定时任务拉取 Redis

上游推单给推单系统后，推单系统把订单推送到配单系统时，配单拉取数据是用定时任务每 2 秒从 Redis 里面拉取 200 条。

以及：
推送服务定时任务批量拉取 Redis 数据；
推单系统定时任务异步线程批量拉取 Redis 里上游回调的数据。
都是 每 2 秒拉取 200 条。

### Rocket MQ

## sql 优化

### 代码层面

for 循环操作数据库改为先用 for 循环组装数据，然后写一条批量 sql 操作，减少和数据库交互次数。
我曾测过两个的效率，在 100 条数据更新的情况下，批量操作大概只需要几毫秒，而 for 循环操作数据库大概需要几百毫秒。
如果一条 sql 需要连表查询，效率会急剧降低，我们可以分别查各表的数据，然后用代码做 join 操作，组装数据。

### 业务层面

使用 记录表 + 消费表 的形式，用 delete 替换 update。

### 索引层面

简历合适的索引，减少回表次数，杜绝锁表的操作。

## 幂等

一锁二判三更新；
MySQL 唯一索引做兜底。

## 设计模式

### 单例模式

线程池

## 服务配置

4 核 8g

推单系统三个服务：
主服务（集群 3台机器）；
推送服务；
后台服务。
主服务集群部署在 3 台机器，推送服务和后台服务部署在同 1 台机器。

配单系统也有三个服务，同样的配置。

Redis 和 MySQL 用的是阿里云（推单系统和配单系统都是 2 台主从）。

### MySQL 和 Redis 为什么用阿里云？

阿里云的 MySQL 非常稳定，如果是自己的机房可能会断电、断网，用阿里云很少会有这种问题，虽然可能会网络抖动，但起码是稳定的（钱到位）；
阿里云 MySQL 的控制台查死锁、慢 sql 特别方便；
阿里云 MySQL 的摘备功能特别好用，直接点那个按钮，可以选择恢复到什么时候的数据（我们有个线上真实案例，当时组长迁数据的时候，点复制时不小心点成删除了，生产数据直接被删了，就是通过这种方式恢复的。如果没有运维的情况下，发生这种事故是很可怕的）。

## 压测

QPS：使用 jemeter。

